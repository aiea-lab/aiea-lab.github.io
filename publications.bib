@inproceedings{xu2022framework,
  title={A Framework for Generating Dangerous Scenes for Testing Robustness},
  author={Xu, Shengjie and Mi, Lan and Gilpin, Leilani H},
  booktitle={Progress and Challenges in Building Trustworthy Embodied AI},
  date={2022-11-23},
url_pdf       = {https://openreview.net/pdf?id=ZjN2AuXgu1},        
  abstract  = {Benchmark datasets for autonomous driving, such as KITTI, nuScenes, Argoverse, or Waymo are realistic but designed to be faultless. These datasets do not contain errors, difficult driving maneuvers, or other corner cases. We propose a framework for perturbing autonomous vehicle datasets, the DANGER framework, which generates edge-case images on top of current autonomous driving datasets. The input to DANGER are photorealistic datasets from real driving scenarios. We present the DANGER algorithm for vehicle position manipulation and the interface towards the renderer module, and present five scenario-level dangerous primitives generation applied to the virtual KITTI and virtual KITTI 2 datasets. Our experiments prove that DANGER can be used as a framework for expanding the current datasets to cover generative while realistic and anomalous corner cases.}
}

@article{macbeth2022proceedings,
  title={Proceedings of the Tenth Annual Conference on Advances in Cognitive Systems},
  author={Macbeth, Jamie C and Gilpin, Leilani and Cox, Michael T},
  date={2022-11-01},
url_pdf       = {https://scholarworks.smith.edu/cgi/viewcontent.cgi?article=1365&context=csc_facpubs},
  abstract  = {We hope you will enjoy these proceedings of the Tenth Annual Conference on Advances in Cognitive Systems (ACS). The event was the first hybrid meeting of ACS in is history. It took place virtually via Zoom and physically at George Mason University in Arlington, Virginia, from Saturday, November 19, to Tuesday, November 22, 2022. We are so delighted to be part of continuing this important venue that focuses on the original long-standing goals and challenges of artificial intelligence research. The conference program consisted of 37 papers consisting of ten 30-minute long talks, fifteen 20-minute short talks, twelve posters, three invited talks, two workshops, and a community panel discussion. This year’s Simon Prize Lecture was delivered by Anthony Cohn, Professor of Automated Reasoning at the University of Leeds. More than 200 people attended both in person and virtually. We were fortunate to have the organizational contributions of our local chair, Mihai Boicu of George Mason University, and of the members of the ACS Organizing Committee: Paul Bello, Pat Langley, Sergei Nirenburg, and Matt Klenk. We also thank Mark Burstein, Ken Forbus, Henry Lieberman, Marge McShane, John Laird, and Alexis Kilayko for their many suggestions and contributions. Finally we thank our sponsors, Smart Information Flow Technologies (SIFT) and George Mason University's Institute for Digital Innovation. The latter provided space and technical support that allowed us to produce a high-quality in-person and virtual event. We invite you to visit the Advances in Cognitive Systems YouTube channel (https://www.youtube.com/ @advancesincognitivesystems4184) which has videos of the presentations, invited talks, tutorials, and the panel discussion.}
}

@article{wurman2022outracing,
  title={Outracing champion Gran Turismo drivers with deep reinforcement learning},
  author={Wurman, Peter R and Barrett, Samuel and Kawamoto, Kenta and MacGlashan, James and Subramanian, Kaushik and Walsh, Thomas J and Capobianco, Roberto and Devlic, Alisa and Eckert, Franziska and Fuchs, Florian and others},
  journal={Nature},
  volume={602},
  number={7896},
  pages={223--228},
  date={2022-02-10},
  publisher={Nature Publishing Group UK London},
url_pdf       = {https://www.nature.com/articles/s41586-021-04357-7},
  abstract  = {Many potential applications of artificial intelligence involve making real-time decisions in physical systems while interacting with humans. Automobile racing represents an extreme example of these conditions; drivers must execute complex tactical manoeuvres to pass or block opponents while operating their vehicles at their traction limits1. Racing simulations, such as the PlayStation game Gran Turismo, faithfully reproduce the non-linear control challenges of real race cars while also encapsulating the complex multi-agent interactions. Here we describe how we trained agents for Gran Turismo that can compete with the world’s best e-sports drivers. We combine state-of-the-art, model-free, deep reinforcement learning algorithms with mixed-scenario training to learn an integrated control policy that combines exceptional speed with impressive tactics. In addition, we construct a reward function that enables the agent to be competitive while adhering to racing’s important, but under-specified, sportsmanship rules. We demonstrate the capabilities of our agent, Gran Turismo Sophy, by winning a head-to-head competition against four of the world’s best Gran Turismo drivers. By describing how we trained championship-level racers, we demonstrate the possibilities and challenges of using these techniques to control complex dynamical systems in domains where agents must respect imprecisely defined human norms.}
}

@article{gilpin2022explanation,
  title={" Explanation" is Not a Technical Term: The Problem of Ambiguity in XAI},
  author={Gilpin, Leilani H and Paley, Andrew R and Alam, Mohammed A and Spurlock, Sarah and Hammond, Kristian J},
  journal={arXiv preprint arXiv:2207.00007},
  date={2022-06-27},
url_pdf       = {https://arxiv.org/pdf/2207.00007},
  abstract  = {There is broad agreement that Artificial Intelligence (AI) systems, particularly those using Machine Learning (ML), should be able to "explain" their behavior. Unfortunately, there is little agreement as to what constitutes an "explanation." This has caused a disconnect between the explanations that systems produce in service of explainable Artificial Intelligence (XAI) and those explanations that users and other audiences actually need, which should be defined by the full spectrum of functional roles, audiences, and capabilities for explanation. In this paper, we explore the features of explanations and how to use those features in evaluating their utility. We focus on the requirements for explanations defined by their functional role, the knowledge states of users who are trying to understand them, and the availability of the information needed to generate them. Further, we discuss the risk of XAI enabling trust in systems without establishing their trustworthiness and define a critical next step for the field of XAI to establish metrics to guide and ground the utility of system-generated explanations.}
}

@article{badea2022establishing,
  title={Establishing meta-decision-making for AI: an ontology of relevance, representation and reasoning},
  author={Badea, Cosmin and Gilpin, Leilani},
  journal={arXiv preprint arXiv:2210.00608},
  date={2022-10-02},
url_pdf       = {https://arxiv.org/pdf/2210.00608},
  abstract  = {We propose an ontology of building decision-making systems, with the aim of establishing Meta-Decision-Making for Artificial Intelligence (AI), improving autonomy, and creating a framework to build metrics and benchmarks upon. To this end, we propose the three parts of Relevance, Representation, and Reasoning, and discuss their value in ensuring safety and mitigating risk in the context of third wave cognitive systems. Our nomenclature reflects the literature on decision-making, and our ontology allows researchers that adopt it to frame their work in relation to one or more of these parts.}
}

@inproceedings{gilpin2023accountability,
  title={Accountability layers: explaining complex system failures by parts},
  author={Gilpin, Leilani H},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={13},
  pages={15439--15439},
  date={2024-07-15},
url_pdf       = {https://ojs.aaai.org/index.php/AAAI/article/view/26806},
  abstract  = {With the rise of AI used for critical decision-making, many important predictions are made by complex and opaque AI algorithms. The aim of eXplainable Artificial Intelligence (XAI) is to make these opaque decision-making algorithms more transparent and trustworthy. This is often done by constructing an``explainable model''for a single modality or subsystem. However, this approach fails for complex systems that are made out of multiple parts. In this paper, I discuss how to explain complex system failures. I represent a complex machine as a hierarchical model of introspective sub-systems working together towards a common goal. The subsystems communicate in a common symbolic language. This work creates a set of explanatory accountability layers for trustworthy AI.}
}

@article{barbosa2023semi,
  title={Semi-Automated Synthesis of Driving Rules},
  author={Barbosa, Diego Ortiz and Gilpin, Leilani and Cardenas, Alvaro A},
  date={2023-02-27},
url_pdf       = {https://neurosymbolic-ai-journal.com/system/files/nai-paper-736.pdf},
  abstract  = {Autonomous vehicles must operate in a complex environment with various social norms and expectations. While most of the work on securing autonomous vehicles has focused on safety, we argue that we also need to monitor for deviations from various societal “common sense” rules to identify attacks against autonomous systems. In this paper, we provide a preliminary approach to encoding and understanding these common-sense driving behaviors by semi-automatically extracting rules from driving manuals. We encode our driving rules in a formal specification and make our rules available online for other researchers.}
}

@article{mitra2023xaisuite,
  title={The XAISuite framework and the implications of explanatory system dissonance},
  author={Mitra, Shreyan and Gilpin, Leilani},
  journal={arXiv preprint arXiv:2304.08499},
  date={2023-04-15},
url_pdf       = {https://arxiv.org/pdf/2304.08499},
  abstract  = {Explanatory systems make machine learning models more transparent. However, they are often inconsistent. In order to quantify and isolate possible scenarios leading to this discrepancy, this paper compares two explanatory systems, SHAP and LIME, based on the correlation of their respective importance scores using 14 machine learning models (7 regression and 7 classification) and 4 tabular datasets (2 regression and 2 classification). We make two novel findings. Firstly, the magnitude of importance is not significant in explanation consistency. The correlations between SHAP and LIME importance scores for the most important features may or may not be more variable than the correlation between SHAP and LIME importance scores averaged across all features. Secondly, the similarity between SHAP and LIME importance scores cannot predict model accuracy. In the process of our research, we construct an open-source library, XAISuite, that unifies the process of training and explaining models. Finally, this paper contributes a generalized framework to better explain machine learning models and optimize their performance.}
}

@article{amos2023anticipatory,
  title={The anticipatory paradigm},
  author={Amos-Binks, Adam and Dannenhauer, Dustin and Gilpin, Leilani H},
  journal={AI Magazine},
  volume={44},
  number={2},
  pages={133--143},
  date={2023-06-29},
  publisher={Wiley Online Library},
url_pdf       = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/aaai.12098},
  abstract  = {Anticipatory thinking is necessary for managing risk in the safety‐ and mission‐critical domains where AI systems are being deployed. We analyze the intersection of anticipatory thinking, the optimization paradigm, and metaforesight to advance our understanding of AI systems and their adaptive capabilities when encountering low‐likelihood/high‐impact risks. We describe this intersection as the anticipatory paradigm. We detail these challenges in concrete examples and propose new types of anticipatory thinking, towards a paradigm shift in how AI systems are evaluated.}
}

@article{amos2023anticipatory,
  title={Anticipatory thinking challenges in open worlds: Risk management},
  author={Amos-Binks, Adam and Dannenhauer, Dustin and Gilpin, Leilani H},
  journal={arXiv preprint arXiv:2306.13157},
  date={2023-06-22},
url_pdf       = {https://arxiv.org/pdf/2306.13157},
  abstract  = {Anticipatory thinking drives our ability to manage risk - identification and mitigation - in everyday life, from bringing an umbrella when it might rain to buying car insurance. As AI systems become part of everyday life, they too have begun to manage risk. Autonomous vehicles log millions of miles, StarCraft and Go agents have similar capabilities to humans, implicitly managing risks presented by their opponents. To further increase performance in these tasks, out-of-distribution evaluation can characterize a model's bias, what we view as a type of risk management. However, learning to identify and mitigate low-frequency, high-impact risks is at odds with the observational bias required to train machine learning models. StarCraft and Go are closed-world domains whose risks are known and mitigations well documented, ideal for learning through repetition. Adversarial filtering datasets provide difficult examples but are laborious to curate and static, both barriers to real-world risk management. Adversarial robustness focuses on model poisoning under the assumption there is an adversary with malicious intent, without considering naturally occurring adversarial examples. These methods are all important steps towards improving risk management but do so without considering open-worlds. We unify these open-world risk management challenges with two contributions. The first is our perception challenges, designed for agents with imperfect perceptions of their environment whose consequences have a high impact. Our second contribution are cognition challenges, designed for agents that must dynamically adjust their risk exposure as they identifynew risks and learn new mitigations. Our goal with these challenges is to spur research into solutions that assess and improve the anticipatory thinking required by AI agents to manage risk in open-worlds and ultimately the real-world.}
}

@article{jenkins2023separating,
  title={Separating facts and evaluation: motivation, account, and learnings from a novel approach to evaluating the human impacts of machine learning},
  author={Jenkins, Ryan and Hammond, Kristian and Spurlock, Sarah and Gilpin, Leilani},
  journal={AI \& SOCIETY},
  volume={38},
  number={4},
  pages={1415--1428},
  date={2023-08-01},
  publisher={Springer},
url_pdf       = {https://link.springer.com/article/10.1007/s00146-022-01417-y},
  abstract  = {In this paper, we outline a new method for evaluating the human impact of machine-learning (ML) applications. In partnership with Underwriters Laboratories Inc., we have developed a framework to evaluate the impacts of a particular use of machine learning that is based on the goals and values of the domain in which that application is deployed. By examining the use of artificial intelligence (AI) in particular domains, such as journalism, criminal justice, or law, we can develop more nuanced and practically relevant understandings of key ethical guidelines for artificial intelligence. By decoupling the extraction of the facts of the matter from the evaluation of the impact of the resulting systems, we create a framework for the process of assessing impact that has two distinctly different phases.}
}

@article{subramanian2023convolutional,
  title={Convolutional neural network model for diabetic retinopathy feature extraction and classification},
  author={Subramanian, Sharan and Gilpin, Leilani H},
  journal={arXiv preprint arXiv:2310.10806},
  date={2023-10-16},
url_pdf       = {https://arxiv.org/pdf/2310.10806},
  abstract  = {The application of Artificial Intelligence in the medical market brings up increasing concerns but aids in more timely diagnosis of silent progressing diseases like Diabetic Retinopathy. In order to diagnose Diabetic Retinopathy (DR), ophthalmologists use color fundus images, or pictures of the back of the retina, to identify small distinct features through a difficult and time-consuming process. Our work creates a novel CNN model and identifies the severity of DR through fundus image input. We classified 4 known DR features, including micro-aneurysms, cotton wools, exudates, and hemorrhages, through convolutional layers and were able to provide an accurate diagnostic without additional user input. The proposed model is more interpretable and robust to overfitting. We present initial results with a sensitivity of 97% and an accuracy of 71%. Our contribution is an interpretable model with similar accuracy to more complex models. With that, our model advances the field of DR detection and proves to be a key step towards AI-focused medical diagnosis.}
}

@article{huang2023can,
  title={Can large language models explain themselves? a study of llm-generated self-explanations},
  author={Huang, Shiyuan and Mamidanna, Siddarth and Jangam, Shreedhar and Zhou, Yilun and Gilpin, Leilani H},
  journal={arXiv preprint arXiv:2310.11207},
  date={2023-10-17},
url_pdf       = {https://arxiv.org/pdf/2310.11207},
  abstract  = {Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce "helpful" responses, they can and often will produce explanations along with the response, which we call self-explanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentiment-laden words such as "fantastic" and "memorable" in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit the self-explanations, evaluate their faithfulness on a set of evaluation metrics, and compare them to traditional explanation methods such as occlusion or LIME saliency maps. Through an extensive set of experiments, we find that ChatGPT's self-explanations perform on par with traditional ones, but are quite different from them according to various agreement metrics, meanwhile being much cheaper to produce (as they are generated along with the prediction). In addition, we identified several interesting characteristics of them, which prompt us to rethink many current model interpretability practices in the era of ChatGPT(-like) LLMs.}
}

@article{la2023towards,
  title={Towards a fuller understanding of neurons with clustered compositional explanations},
  author={La Rosa, Biagio and Gilpin, Leilani and Capobianco, Roberto},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={70333--70354},
  date={2023-12-15},
url_pdf       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/debd0ae2083160397a22a4a8831c7230-Paper-Conference.pdf},
  abstract  = {Compositional Explanations is a method for identifying logical formulas of concepts that approximate the neurons' behavior. However, these explanations are linked to the small spectrum of neuron activations (ie, the highest ones) used to check the alignment, thus lacking completeness. In this paper, we propose a generalization, called Clustered Compositional Explanations, that combines Compositional Explanations with clustering and a novel search heuristic to approximate a broader spectrum of the neuron behavior. We define and address the problems connected to the application of these methods to multiple ranges of activations, analyze the insights retrievable by using our algorithm, and propose desiderata qualities that can be used to study the explanations returned by different algorithms.}
}

@inproceedings{ortiz2024semi,
  title={Semi-Automated Synthesis of Driving Rules},
  author={Ortiz, Diego and Gilpin, Leilani and Cardenas, Alvaro A},
  date={2023-01-01},
  organization={ISOC NDSS},
url_pdf       = {https://par.nsf.gov/servlets/purl/10470119},
  abstract  = {Autonomous vehicles must operate in a complex environment with various social norms and expectations. While most of the work on securing autonomous vehicles has focused on safety, we argue that we also need to monitor for deviations from various societal “common sense” rules to identify attacks against autonomous systems. In this paper, we provide a first approach to encoding and understanding these common-sense driving behaviors by semi-automatically extracting rules from driving manuals. We encode our driving rules in a formal specification and make our rules available online for other researchers.}
}

@inproceedings{mitra2024novel,
  title={A novel post-hoc explanation comparison metric and applications},
  author={Mitra, Shreyan and Gilpin, Leilani},
  booktitle={International Conference on Pattern Recognition and Artificial Intelligence},
  pages={427--446},
  date={2024-06-18},
  organization={Springer},
url_pdf       = {https://arxiv.org/pdf/2311.10811},
  abstract  = {Explanatory systems make the behavior of machine learning models more transparent, but are often inconsistent. To quantify the differences between explanatory systems, this paper presents the Shreyan Distance, a novel metric based on the weighted difference between ranked feature importance lists produced by such systems. This paper uses the Shreyan Distance to compare two explanatory systems, SHAP and LIME, for both regression and classification learning tasks. Because we find that the average Shreyan Distance varies significantly between these two tasks, we conclude that consistency between explainers not only depends on inherent properties of the explainers themselves, but also the type of learning task. This paper further contributes the XAISuite library, which integrates the Shreyan distance algorithm into machine learning pipelines.}
}

@misc{douglas2024user,
  title={User interface for operating artificial intelligence experiments},
  author={Douglas, Rory and Whitehead, Dion and Barrett, Leon and Khandelwal, Piyush and Walsh, Thomas and BARRETT, Samuel and Subramanian, Kaushik and MACGLASHAN, James and Gilpin, Leilani and WURMAN, Peter and others},
  date={2024-06-25},
  publisher={Google Patents},
  note={US Patent 12,017,148}
url_pdf       = {https://patentimages.storage.googleapis.com/b5/7f/06/0f45d359b7c0ba/US12017148.pdf},
  abstract  = {A user interface (UI), for analyzing model training runs, tracking and visualizing various aspects of machine learning experiments, can be used when training an artificial intelligent agent in, for example, a racing game environment. The UI can be web-based and can allow researchers to easily see the status of their experiments. The UI can include an experiment synchronized event viewer that can synchronizes visualizations, videos, and timeline/metrics graphs in the experiment. This viewer allows researchers to see how experiments unfold in great detail. The UI can further include experiment event annotations that can generate event annotations. These annotations can be displayed via the synchronized event viewer. The UI can be used to consider consolidated results across experiments and can further consider videos. For example, the UI can provide a reusable dashboard that can capture and compare metrics across multiple experiments.}
}

@article{wellington2024slug,
  title={Slug Mobile: Test-Bench for RL Testing},
  author={Wellington Morris, Jonathan and Shah, Vishrut and Besanceney, Alex and Shah, Daksh and Gilpin, Leilani H},
  journal={arXiv e-prints},
  pages={arXiv--2409},
  date={2024-08-01},
url_pdf       = {https://arxiv.org/pdf/2409.10532},
  abstract  = {Sim-to real gap in Reinforcement Learning is when a model trained in a simulator does not translate to the real world. This is a problem for Autonomous Vehicles (AVs) as vehicle dynamics can vary from simulation to reality, and also from vehicle to vehicle. Slug Mobile is a one tenth scale autonomous vehicle created to help address the sim-to-real gap for AVs by acting as a test-bench to develop models that can easily scale from one vehicle to another. In addition to traditional sensors found in other one tenth scale AVs, we have also included a Dynamic Vision Sensor so we can train Spiking Neural Networks running on neuromorphic hardware.}
}

@article{morris2024slug,
  title={Slug Mobile: Test-Bench for RL Testing},
  author={Morris, Jonathan Wellington and Shah, Vishrut and Besanceney, Alex and Shah, Daksh and Gilpin, Leilani H},
  journal={arXiv preprint arXiv:2409.10532},
  date={2024-08-31},
url_pdf       = {https://arxiv.org/pdf/2409.10532},
  abstract  = {Sim-to real gap in Reinforcement Learning is when a model trained in a simulator does not translate to the real world. This is a problem for Autonomous Vehicles (AVs) as vehicle dynamics can vary from simulation to reality, and also from vehicle to vehicle. Slug Mobile is a one tenth scale autonomous vehicle created to help address the sim-to-real gap for AVs by acting as a test-bench to develop models that can easily scale from one vehicle to another. In addition to traditional sensors found in other one tenth scale AVs, we have also included a Dynamic Vision Sensor so we can train Spiking Neural Networks running on neuromorphic hardware.}
}

@inproceedings{vakharia2024proslm,
  title={Proslm: A prolog synergized language model for explainable domain specific knowledge based question answering},
  author={Vakharia, Priyesh and Kufeldt, Abigail and Meyers, Max and Lane, Ian and Gilpin, Leilani H},
  booktitle={International Conference on Neural-Symbolic Learning and Reasoning},
  pages={291--304},
  date={2024-09-09},
  organization={Springer},
url_pdf       = {https://arxiv.org/pdf/2409.11589?},
  abstract  = {Neurosymbolic approaches can add robustness to opaque neural systems by incorporating explainable symbolic representations. However, previous approaches have not used formal logic to contextualize queries to and validate outputs of large language models (LLMs). We propose ProSLM, a novel neurosymbolic framework, to improve the robustness and reliability of LLMs in question-answering tasks. We provide ProSLM with a domain-specific knowledge base, a logical reasoning system, and an integration to an existing LLM. This framework has two capabilities (1) context gathering: generating explainable and relevant context for a given query, and (2) validation: confirming and validating the factual accuracy of a statement in accordance with a knowledge base (KB). Our work opens a new area of neurosymbolic generative AI text validation and user personalization.}
}

@article{zhu2024autonomous,
  title={Autonomous driving with spiking neural networks},
  author={Zhu, Rui-Jie and Wang, Ziqing and Gilpin, Leilani and Eshraghian, Jason},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={136782--136804},
  date={2024-12-16},
url_pdf       = {https://proceedings.neurips.cc/paper_files/paper/2024/file/f7344147dbd1607deac3a7e5f33a23aa-Paper-Conference.pdf},
  abstract  = {Autonomous driving demands an integrated approach that encompasses perception, prediction, and planning, all while operating under strict energy constraints to enhance scalability and environmental sustainability. We present Spiking Autonomous Driving (SAD), the first unified Spiking Neural Network (SNN) to address the energy challenges faced by autonomous driving systems through its event-driven and energy-efficient nature. SAD is trained end-to-end and consists of three main modules: perception, which processes inputs from multi-view cameras to construct a spatiotemporal bird's eye view; prediction, which utilizes a novel dual-pathway with spiking neurons to forecast future states; and planning, which generates safe trajectories considering predicted occupancy, traffic rules, and ride comfort. Evaluated on the nuScenes dataset, SAD achieves competitive performance in perception, prediction, and planning tasks, while drawing upon the energy efficiency of SNNs. This work highlights the potential of neuromorphic computing to be applied to energy-efficient autonomous driving, a critical step toward sustainable and safety-critical automotive technology. Our code is available at https://github. com/ridgerchu/SAD.}
}

@article{liu2024right,
  title={Right this way: Can VLMs Guide Us to See More to Answer Questions?},
  author={Liu, Li and Yang, Diji and Zhong, Sijia and Tholeti, Kalyana Suma Sree and Ding, Lei and Zhang, Yi and Gilpin, Leilani},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={132946--132976},
  date={2024-12-16},
url_pdf       = {https://proceedings.neurips.cc/paper_files/paper/2024/file/efe4e50d492fedc0dfd2959f3320a974-Paper-Conference.pdf},
  abstract  = {In question-answering scenarios, humans can assess whether the available information is sufficient and seek additional information if necessary, rather than providing a forced answer. In contrast, Vision Language Models (VLMs) typically generate direct, one-shot responses without evaluating the sufficiency of the information. To investigate this gap, we identify a critical and challenging task in the Visual Question Answering (VQA) scenario: can VLMs indicate how to adjust an image when the visual information is insufficient to answer a question? This capability is especially valuable for assisting visually impaired individuals who often need guidance to capture images correctly. To evaluate this capability of current VLMs, we introduce a human-labeled dataset as a benchmark for this task. Additionally, we present an automated framework that generates synthetic training data by simulating``where to know''scenarios. Our empirical results show significant performance improvements in mainstream VLMs when fine-tuned with this synthetic data. This study demonstrates the potential to narrow the gap between information assessment and acquisition in VLMs, bringing their performance closer to humans.Our dataset and code are available at: https://github.com/LeoLee7/Directional_guidance.}
}

@article{chauhan2025vfsi,
  title={VFSI: Validity First Spatial Intelligence for Constraint-Guided Traffic Diffusion},
  author={Chauhan, Kargi and Gilpin, Leilani H},
  journal={arXiv preprint arXiv:2509.23971},
  date={2025-09-28},
url_pdf       = {https://arxiv.org/pdf/2509.23971?},
  abstract  = {Modern diffusion models generate realistic traffic simulations but systematically violate physical constraints. In a large-scale evaluation of SceneDiffuser++, a state-of-the-art traffic simulator, we find that 50% of generated trajectories violate basic physical laws - vehicles collide, drive off roads, and spawn inside buildings. This reveals a fundamental limitation: current models treat physical validity as an emergent property rather than an architectural requirement. We propose Validity-First Spatial Intelligence (VFSI), which enforces constraints through energy-based guidance during diffusion sampling, without model retraining. By incorporating collision avoidance and kinematic constraints as energy functions, we guide the denoising process toward physically valid trajectories. Across 200 urban scenarios from the Waymo Open Motion Dataset, VFSI reduces collision rates by 67% (24.6% to 8.1%) and improves overall validity by 87% (50.3% to 94.2%), while simultaneously improving realism metrics (ADE: 1.34m to 1.21m). Our model-agnostic approach demonstrates that explicit constraint enforcement during inference is both necessary and sufficient for physically valid traffic simulation.}
}

@article{peiyu2025follow,
  title={Follow My Lead: Logical Fallacy Classification with Knowledge-Augmented LLMs},
  author={Peiyu Wang, Olivia and Bansal, Tashvi and Bai, Ryan and Chui, Emily M and Gilpin, Leilani H},
  journal={arXiv e-prints},
  pages={arXiv--2510},
  date={2025-10-01},
url_pdf       = {https://arxiv.org/pdf/2510.09970},
  abstract  = {Large Language Models (LLMs) suffer from critical reasoning gaps, including a tendency to hallucinate and poor accuracy in classifying logical fallacies. This limitation stems from their default System 1 processing, which is fast and intuitive, whereas reliable reasoning requires the deliberate, effortful System 2 approach (Kahneman, 2011; Li et al., 2025). Since full System 2 training is often prohibitively expensive, we explore a low-cost, instruction-based intervention to bridge this gap. Our methodology introduces a novel stepwise instruction dataset that decomposes fallacy classification into a series of atomic procedural steps (simple binary questions). We further augment this with a final verification step where models consult a relational knowledge graph of related fallacies. This procedural, rule-based intervention yields a significant improvement in LLM logical fallacy classification. Crucially, the approach also provides enhanced transparency into the LLMs' decision-making, highlighting a practical pathway for Neuro-symbolic architectures to address LLM reasoning deficits.}
}

@article{manikandan2025explore,
  title={Explore the Loss space with Hill-ADAM},
  author={Manikandan, Meenakshi and Gilpin, Leilani},
  journal={arXiv preprint arXiv:2510.03613},
  date={2025-10-04},
url_pdf       = {https://arxiv.org/pdf/2510.03613?},
  abstract  = {This paper introduces Hill-ADAM. Hill-ADAM is an optimizer with its focus towards escaping local minima in prescribed loss landscapes to find the global minimum. Hill-ADAM escapes minima by deterministically exploring the state space. This eliminates uncertainty from random gradient updates in stochastic algorithms while seldom converging at the first minimum that visits. In the paper we first derive an analytical approximation of the ADAM Optimizer step size at a particular model state. From there define the primary condition determining ADAM limitations in escaping local minima. The proposed optimizer algorithm Hill-ADAM alternates between error minimization and maximization. It maximizes to escape the local minimum and minimizes again afterward. This alternation provides an overall exploration throughout the loss space. This allows the deduction of the global minimum's state. Hill-ADAM was tested with 5 loss functions and 12 amber-saturated to cooler-shade image color correction instances.}
}

@article{wang2025follow,
  title={Follow My Lead: Logical Fallacy Classification with Knowledge-Augmented LLMs},
  author={Wang, Olivia Peiyu and Bansal, Tashvi and Bai, Ryan and Chui, Emily M and Gilpin, Leilani H},
  journal={arXiv preprint arXiv:2510.09970},
  date={2025-10-11},
url_pdf       = {https://arxiv.org/pdf/2510.09970?},
  abstract  = {Large Language Models (LLMs) suffer from critical reasoning gaps, including a tendency to hallucinate and poor accuracy in classifying logical fallacies. This limitation stems from their default System 1 processing, which is fast and intuitive, whereas reliable reasoning requires the deliberate, effortful System 2 approach (Kahneman, 2011; Li et al., 2025). Since full System 2 training is often prohibitively expensive, we explore a low-cost, instruction-based intervention to bridge this gap. Our methodology introduces a novel stepwise instruction dataset that decomposes fallacy classification into a series of atomic procedural steps (simple binary questions). We further augment this with a final verification step where models consult a relational knowledge graph of related fallacies. This procedural, rule-based intervention yields a significant improvement in LLM logical fallacy classification. Crucially, the approach also provides enhanced transparency into the LLMs' decision-making, highlighting a practical pathway for Neuro-symbolic architectures to address LLM reasoning deficits.}
}
